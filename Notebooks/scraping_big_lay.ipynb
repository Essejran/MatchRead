{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Need To Run This Notebook!\n",
    "This notebook is only there to document what was done during the data preprocessing.\n",
    "In the absolute exceptional emergency of needing to run it again, please check file paths.\n",
    "Some may need adding \"../\" before them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset books.csv loaded successfully\n",
      "isbn column dropped, isbn13 column kept\n",
      "Language codes eng, en-US, en-GB, en-CA unified to 'en'\n",
      "Missing publication dates filled in\n",
      "2nd (large) dataset is being downloaded...\n",
      "Dataset downloaded and saved as data/books2.csv\n",
      "Dataset downloaded and saved as data/users.csv\n",
      "Dataset downloaded and saved as data/ratings.csv\n",
      "Pandas dataframes (books_df, books_big, users, ratings) loaded successfully\n",
      "Columns renamed and dates converted to dtype: datetime\n",
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# %run import_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'kind': 'books#volumes', 'totalItems': 1, 'items': [{'kind': 'books#volume', 'id': 'DlQbmJc5WlQC', 'etag': '6/c0TK9Vhc0', 'selfLink': 'https://www.googleapis.com/books/v1/volumes/DlQbmJc5WlQC', 'volumeInfo': {'title': 'Fantastic Mr. Fox', 'authors': ['Roald Dahl'], 'publisher': 'Puffin', 'publishedDate': '1988', 'description': 'In this book you will find: Boggis an enormously fat man, a chicken farmer and a mean man. Bunce, a pot bellied dwarf, a duck-and-goose farmer and a nasty man. Bean, a thin man, a turkey-and-apple farmer and a beastly man. Badger, the most respectable and well-behaved animal in the district. Rat, a rude creature and a drunkard, and also a Mrs. Fox and her four children.', 'industryIdentifiers': [{'type': 'ISBN_10', 'identifier': '0140328726'}, {'type': 'ISBN_13', 'identifier': '9780140328721'}], 'readingModes': {'text': False, 'image': False}, 'pageCount': 100, 'printType': 'BOOK', 'categories': ['Juvenile Fiction'], 'maturityRating': 'NOT_MATURE', 'allowAnonLogging': False, 'contentVersion': '0.4.2.0.preview.0', 'panelizationSummary': {'containsEpubBubbles': False, 'containsImageBubbles': False}, 'imageLinks': {'smallThumbnail': 'http://books.google.com/books/content?id=DlQbmJc5WlQC&printsec=frontcover&img=1&zoom=5&source=gbs_api', 'thumbnail': 'http://books.google.com/books/content?id=DlQbmJc5WlQC&printsec=frontcover&img=1&zoom=1&source=gbs_api'}, 'language': 'en', 'previewLink': 'http://books.google.de/books?id=DlQbmJc5WlQC&dq=isbn:9780140328721&hl=&cd=1&source=gbs_api', 'infoLink': 'http://books.google.de/books?id=DlQbmJc5WlQC&dq=isbn:9780140328721&hl=&source=gbs_api', 'canonicalVolumeLink': 'https://books.google.com/books/about/Fantastic_Mr_Fox.html?hl=&id=DlQbmJc5WlQC'}, 'saleInfo': {'country': 'DE', 'saleability': 'NOT_FOR_SALE', 'isEbook': False}, 'accessInfo': {'country': 'DE', 'viewability': 'NO_PAGES', 'embeddable': False, 'publicDomain': False, 'textToSpeechPermission': 'ALLOWED', 'epub': {'isAvailable': False}, 'pdf': {'isAvailable': False}, 'webReaderLink': 'http://play.google.com/books/reader?id=DlQbmJc5WlQC&hl=&source=gbs_api', 'accessViewStatus': 'NONE', 'quoteSharingAllowed': False}, 'searchInfo': {'textSnippet': 'In this book you will find: Boggis an enormously fat man, a chicken farmer and a mean man.'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Test the Google Books API key with a sample request\n",
    "\n",
    "import requests\n",
    "\n",
    "API_KEY = \"AIzaSyDcAxovkBpRGJgsR6BGTZCGodOHmoU2oEM\"\n",
    "url = f\"https://www.googleapis.com/books/v1/volumes?q=isbn:9780140328721&key={API_KEY}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'ISBN:9780140328721': {'url': 'https://openlibrary.org/books/OL7353617M/Fantastic_Mr._Fox', 'key': '/books/OL7353617M', 'title': 'Fantastic Mr. Fox', 'authors': [{'url': 'https://openlibrary.org/authors/OL34184A/Roald_Dahl', 'name': 'Roald Dahl'}], 'number_of_pages': 96, 'identifiers': {'goodreads': ['1507552'], 'librarything': ['6446'], 'isbn_10': ['0140328726'], 'isbn_13': ['9780140328721'], 'openlibrary': ['OL7353617M']}, 'publishers': [{'name': 'Puffin'}], 'publish_date': 'October 1, 1988', 'subjects': [{'name': 'Animals', 'url': 'https://openlibrary.org/subjects/animals'}, {'name': 'Hunger', 'url': 'https://openlibrary.org/subjects/hunger'}, {'name': 'Open Library Staff Picks', 'url': 'https://openlibrary.org/subjects/open_library_staff_picks'}, {'name': 'Juvenile fiction', 'url': 'https://openlibrary.org/subjects/juvenile_fiction'}, {'name': \"Children's stories, English\", 'url': \"https://openlibrary.org/subjects/children's_stories,_english\"}, {'name': 'Foxes', 'url': 'https://openlibrary.org/subjects/foxes'}, {'name': 'Fiction', 'url': 'https://openlibrary.org/subjects/fiction'}, {'name': 'Zorros', 'url': 'https://openlibrary.org/subjects/zorros'}, {'name': 'Ficción juvenil', 'url': 'https://openlibrary.org/subjects/ficción_juvenil'}, {'name': 'Tunnels', 'url': 'https://openlibrary.org/subjects/tunnels'}, {'name': 'Interviews', 'url': 'https://openlibrary.org/subjects/interviews'}, {'name': 'Farmers', 'url': 'https://openlibrary.org/subjects/farmers'}, {'name': \"Children's stories\", 'url': \"https://openlibrary.org/subjects/children's_stories\"}, {'name': 'Rats', 'url': 'https://openlibrary.org/subjects/rats'}, {'name': 'Welsh Authors', 'url': 'https://openlibrary.org/subjects/welsh_authors'}, {'name': 'English Authors', 'url': 'https://openlibrary.org/subjects/english_authors'}, {'name': 'Thieves', 'url': 'https://openlibrary.org/subjects/thieves'}, {'name': 'Tricksters', 'url': 'https://openlibrary.org/subjects/tricksters'}, {'name': 'Badgers', 'url': 'https://openlibrary.org/subjects/badgers'}, {'name': \"Children's fiction\", 'url': \"https://openlibrary.org/subjects/children's_fiction\"}, {'name': 'Foxes, fiction', 'url': 'https://openlibrary.org/subjects/foxes,_fiction'}, {'name': 'Underground', 'url': 'https://openlibrary.org/subjects/underground'}, {'name': 'Renards', 'url': 'https://openlibrary.org/subjects/renards'}, {'name': 'Romans, nouvelles, etc. pour la jeunesse', 'url': 'https://openlibrary.org/subjects/romans,_nouvelles,_etc._pour_la_jeunesse'}, {'name': \"Children's literature\", 'url': \"https://openlibrary.org/subjects/children's_literature\"}, {'name': 'Plays', 'url': 'https://openlibrary.org/subjects/plays'}, {'name': \"Children's plays\", 'url': \"https://openlibrary.org/subjects/children's_plays\"}, {'name': \"Children's stories, Welsh\", 'url': \"https://openlibrary.org/subjects/children's_stories,_welsh\"}, {'name': 'Agriculteurs', 'url': 'https://openlibrary.org/subjects/agriculteurs'}, {'name': 'Fantasy fiction', 'url': 'https://openlibrary.org/subjects/fantasy_fiction'}, {'name': \"Children's plays, English\", 'url': \"https://openlibrary.org/subjects/children's_plays,_english\"}], 'subject_places': [{'name': 'English countryside', 'url': 'https://openlibrary.org/subjects/place:english_countryside'}], 'subject_people': [{'name': 'Bean', 'url': 'https://openlibrary.org/subjects/person:bean'}, {'name': 'Boggis', 'url': 'https://openlibrary.org/subjects/person:boggis'}, {'name': 'Bunce', 'url': 'https://openlibrary.org/subjects/person:bunce'}, {'name': 'Mr Fox', 'url': 'https://openlibrary.org/subjects/person:mr_fox'}], 'subject_times': [{'name': '20th Century', 'url': 'https://openlibrary.org/subjects/time:20th_century'}], 'excerpts': [{'text': 'Down in the valley there were three farms.', 'comment': '', 'first_sentence': True}], 'ebooks': [{'preview_url': 'https://archive.org/details/fantasticmrfoxpu00roal', 'availability': 'restricted', 'formats': {}}], 'cover': {'small': 'https://covers.openlibrary.org/b/id/8739161-S.jpg', 'medium': 'https://covers.openlibrary.org/b/id/8739161-M.jpg', 'large': 'https://covers.openlibrary.org/b/id/8739161-L.jpg'}}}\n"
     ]
    }
   ],
   "source": [
    "# Test the Open Library API key with a sample request\n",
    "\n",
    "import requests\n",
    "\n",
    "API_KEY = \"AIzaSyDcAxovkBpRGJgsR6BGTZCGodOHmoU2oEM\"\n",
    "url = f\"https://openlibrary.org/api/books?bibkeys=ISBN:9780140328721&format=json&jscmd=data\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_big.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_big.shape[0]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts = 20\n",
    "len(books_big) % num_parts\n",
    "part_size = len(books_big) // num_parts\n",
    "df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20 = [books_big[i*part_size:(i+1)*part_size] for i in range(num_parts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13568, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from asyncio import Semaphore\n",
    "import logging\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Enable logging for debugging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Set the maximum number of concurrent requests and processing adjustments\n",
    "MAX_CONCURRENT_REQUESTS = 1000  # High concurrency for sending requests\n",
    "semaphore_requests = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# Lower concurrency for processing the fetched data\n",
    "MAX_CONCURRENT_FETCHES = 50  # Adjust this value as needed\n",
    "semaphore_fetches = Semaphore(MAX_CONCURRENT_FETCHES)\n",
    "\n",
    "# Function to fetch data from Open Library\n",
    "async def fetch_data(session, isbn, field):\n",
    "    async with semaphore_requests:\n",
    "        url = f'https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}&format=json&jscmd=data'\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                logger.info(f'Fetching ISBN: {isbn} with status: {response.status}')\n",
    "                if response.status == 429:  # Rate limit exceeded\n",
    "                    logger.warning(f'Rate limit hit for ISBN: {isbn}, retrying after 120 seconds')\n",
    "                    await asyncio.sleep(120)  # Wait before retrying\n",
    "                    return await fetch_data(session, isbn, field)  # Retry\n",
    "                elif response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    return data, isbn, field\n",
    "                else:\n",
    "                    logger.error(f'Error fetching ISBN: {isbn}, status code: {response.status}')\n",
    "                    return None, isbn, field\n",
    "        except Exception as e:\n",
    "            logger.error(f'Exception occurred for ISBN: {isbn}: {e}')\n",
    "            return None, isbn, field\n",
    "\n",
    "# Function to process the data with lower concurrency\n",
    "async def process_data(data, isbn, field):\n",
    "    async with semaphore_fetches:\n",
    "        if data:\n",
    "            key = f'ISBN:{isbn}'\n",
    "            if key in data:\n",
    "                item = data[key]\n",
    "                if field == 'publish_date':\n",
    "                    publish_date = item.get('publish_date', 'No date found')\n",
    "                    return publish_date.split('-')[0] if publish_date != 'No date found' else 'No date found'\n",
    "                elif field == 'genres':\n",
    "                    subjects = item.get('subjects', [])\n",
    "                    genre_names = [subject.get('name', 'Unknown genre') for subject in subjects if isinstance(subject, dict)]\n",
    "                    return ', '.join(genre_names) if genre_names else 'No genres found'\n",
    "            else:\n",
    "                return 'No data found'\n",
    "        else:\n",
    "            return 'Error fetching data'\n",
    "\n",
    "# Function to fetch book information for a DataFrame\n",
    "async def fetch_book_info(df, file_path):\n",
    "    logger.debug(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "    if 'isbn' not in df.columns:\n",
    "        raise ValueError(\"The DataFrame does not contain an 'isbn' column.\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for isbn in df['isbn']:\n",
    "            row = {'isbn': isbn}\n",
    "\n",
    "            # Fetch and process data for each field\n",
    "            for field in ['publish_date', 'genres']:\n",
    "                data, isbn, field = await fetch_data(session, isbn, field)\n",
    "                result = await process_data(data, isbn, field)\n",
    "                row[field] = result\n",
    "\n",
    "            # Append the row to the CSV file immediately\n",
    "            result_df = pd.DataFrame([row])\n",
    "            result_df.to_csv(file_path, mode='a', index=False, header=not pd.io.common.file_exists(file_path))\n",
    "            logger.info(f\"Data for ISBN '{isbn}' appended to CSV.\")\n",
    "\n",
    "    logger.info(\"Finished processing book information for current DataFrame.\")\n",
    "\n",
    "async def main(df, file_path):\n",
    "    logger.info(\"Starting to fetch book information...\")\n",
    "    await fetch_book_info(df, file_path)\n",
    "    logger.info(\"Finished fetching book information.\")\n",
    "\n",
    "# Run the asyncio event loop to fetch the book information\n",
    "loop = asyncio.get_event_loop()\n",
    "file_path = 'data/big_detailed.csv'  # Path to your CSV file\n",
    "\n",
    "for df in [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20]:\n",
    "    loop.run_until_complete(main(df, file_path))\n",
    "    print(\"One fetch done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aiohttp\n",
    "# import asyncio\n",
    "# import pandas as pd\n",
    "# import nest_asyncio\n",
    "# from asyncio import Semaphore\n",
    "# import logging\n",
    "\n",
    "# # Apply nest_asyncio to allow nested event loops\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Enable logging for debugging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Set the maximum number of concurrent requests and processing adjustments\n",
    "# MAX_CONCURRENT_REQUESTS = 1000  # High concurrency for sending requests\n",
    "# semaphore_requests = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# # Lower concurrency for processing the fetched data\n",
    "# MAX_CONCURRENT_FETCHES = 500  # Adjust this value as needed\n",
    "# semaphore_fetches = Semaphore(MAX_CONCURRENT_FETCHES)\n",
    "# # Function to fetch data from Open Library\n",
    "# async def fetch_data(session, isbn, field):\n",
    "\n",
    "#     async with semaphore_requests:\n",
    "#         url = f'https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}&format=json&jscmd=data'\n",
    "#         try:\n",
    "#             async with session.get(url) as response:\n",
    "#                 logger.info(f'Fetching ISBN: {isbn} with status: {response.status}')\n",
    "#                 if response.status == 429:  # Rate limit exceeded\n",
    "#                     logger.warning(f'Rate limit hit for ISBN: {isbn}, retrying after 120 seconds')\n",
    "#                     await asyncio.sleep(120)  # Wait before retrying\n",
    "#                     return await fetch_data(session, isbn, field)  # Retry\n",
    "#                 elif response.status == 200:\n",
    "#                     data = await response.json()\n",
    "#                     return data, isbn, field\n",
    "#                 else:\n",
    "#                     logger.error(f'Error fetching ISBN: {isbn}, status code: {response.status}')\n",
    "#                     return None, isbn, field\n",
    "#         except Exception as e:\n",
    "#             logger.error(f'Exception occurred for ISBN: {isbn}: {e}')\n",
    "#             return None, isbn, field\n",
    "\n",
    "# # Function to process the data with lower concurrency\n",
    "# async def process_data(data, isbn, field):\n",
    "#     async with semaphore_fetches:\n",
    "#         if data:\n",
    "#             key = f'ISBN:{isbn}'\n",
    "#             if key in data:\n",
    "#                 item = data[key]\n",
    "#                 if field == 'publish_date':\n",
    "#                     publish_date = item.get(field, 'No date found')\n",
    "#                     return publish_date.split('-')[0] if publish_date != 'No date found' else 'No date found'\n",
    "#                 else:\n",
    "#                     return item.get(field, 'No data found')\n",
    "#             else:\n",
    "#                 return 'No data found'\n",
    "#         else:\n",
    "#             return 'Error fetching data'\n",
    "\n",
    "# # Function to fetch book information for a DataFrame\n",
    "# async def fetch_book_info(df):\n",
    "#     logger.debug(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "#     if 'isbn' not in df.columns:\n",
    "#         raise ValueError(\"The DataFrame does not contain an 'isbn' column.\")\n",
    "\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         tasks = {\n",
    "#             'publish_date': [],\n",
    "#             'authors': [],\n",
    "#             'title': []\n",
    "#         }\n",
    "\n",
    "#         # Phase 1: Send out all requests\n",
    "#         for isbn in df['isbn']:\n",
    "#             for field in tasks.keys():\n",
    "#                 tasks[field].append(fetch_data(session, isbn, field))\n",
    "        \n",
    "#         # Phase 2: Wait for all requests to complete and then process the results with lower concurrency\n",
    "#         results = {}\n",
    "#         for field, task_list in tasks.items():\n",
    "#             fetch_results = await asyncio.gather(*task_list)\n",
    "#             process_tasks = [process_data(data, isbn, field) for data, isbn, field in fetch_results]\n",
    "#             results[field] = await asyncio.gather(*process_tasks)\n",
    "\n",
    "#     # Add the processed results to the DataFrame\n",
    "#     for field, result in results.items():\n",
    "#         df[field] = result\n",
    "\n",
    "#     return df\n",
    "\n",
    "# async def main(df):\n",
    "#     logger.info(\"Starting to fetch book information...\")\n",
    "#     # Replace `df1` with your actual DataFrame variable\n",
    "#     updated_df = await fetch_book_info(df)\n",
    "#     updated_df.to_csv('data/big_detailed.csv', mode='a', index=False) # save to csv\n",
    "#     logger.info(\"Finished fetching book information.\")\n",
    "\n",
    "# # Run the asyncio event loop to fetch the book information\n",
    "# loop = asyncio.get_event_loop()\n",
    "# for df in [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20]:\n",
    "#     loop.run_until_complete(main(df))\n",
    "#     print(\"one fiftch done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aiohttp\n",
    "# import asyncio\n",
    "# import pandas as pd\n",
    "# import nest_asyncio\n",
    "# from asyncio import Semaphore\n",
    "# import logging\n",
    "\n",
    "# # Apply the nest_asyncio to allow nested event loops\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Enable logging for debugging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Set the maximum number of concurrent requests and rate limit adjustments\n",
    "# MAX_CONCURRENT_REQUESTS = 1000  # Adjust concurrency based on your needs\n",
    "# semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# # Function to fetch publication year from Open Library\n",
    "# async def fetch_data(session, isbn, field, retries=5):\n",
    "#     async with semaphore:\n",
    "#         url = f'https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}&format=json&jscmd=data'\n",
    "#         for attempt in range(retries):\n",
    "#             try:\n",
    "#                 async with session.get(url) as response:\n",
    "#                     logger.info(f'Fetching ISBN: {isbn} with status: {response.status}')\n",
    "                    \n",
    "#                     if response.status == 503:  # Service unavailable\n",
    "#                         wait_time = 2 ** attempt  # Exponential backoff\n",
    "#                         logger.warning(f'Service unavailable for ISBN: {isbn}, retrying in {wait_time} seconds...')\n",
    "#                         await asyncio.sleep(wait_time)\n",
    "#                         continue  # Retry the request\n",
    "\n",
    "#                     elif response.status == 429:  # Rate limit exceeded\n",
    "#                         logger.warning(f'Rate limit hit for ISBN: {isbn}, retrying after 120 seconds...')\n",
    "#                         await asyncio.sleep(120)\n",
    "#                         return await fetch_data(session, isbn, field)  # Retry\n",
    "                    \n",
    "#                     elif response.status == 200:\n",
    "#                         data = await response.json()\n",
    "#                         key = f'ISBN:{isbn}'\n",
    "#                         if key in data:\n",
    "#                             item = data[key]\n",
    "#                             if field == 'publish_date':\n",
    "#                                 publish_date = item.get(field, 'No date found')\n",
    "#                                 return publish_date.split('-')[0] if publish_date != 'No date found' else 'No date found'\n",
    "#                             else:\n",
    "#                                 return item.get(field, 'No data found')\n",
    "#                         else:\n",
    "#                             return 'No data found'\n",
    "#                     else:\n",
    "#                         logger.error(f'Error fetching ISBN: {isbn}, status code: {response.status}')\n",
    "#                         return 'Error fetching data'\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 logger.error(f'Exception occurred for ISBN: {isbn}: {e}')\n",
    "#                 return 'Error fetching data'\n",
    "        \n",
    "#         logger.error(f'Failed to fetch ISBN: {isbn} after {retries} attempts')\n",
    "#         return 'Service unavailable after retries'\n",
    "\n",
    "\n",
    "# # Function to fetch book information for a DataFrame\n",
    "# async def fetch_book_info_for_dataframe(df):\n",
    "#     logger.debug(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "#     if 'isbn' not in df.columns:\n",
    "#         raise ValueError(\"The DataFrame does not contain an 'isbn' column.\")\n",
    "\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         tasks = {\n",
    "#             'publish_date': [],\n",
    "#             'authors': [],\n",
    "#             'title': []\n",
    "#         }\n",
    "        \n",
    "#         for isbn in df['isbn']:\n",
    "#             try:\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(fetch_data(session, isbn, field))\n",
    "#             except ValueError as e:\n",
    "#                 logger.error(f\"Skipping ISBN: {isbn} due to error: {e}\")\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(asyncio.sleep(0))  # Placeholder for skipped tasks\n",
    "\n",
    "#             completed_requests = sum(len(task_list) for task_list in tasks.values())\n",
    "#             logger.debug(f'Completed requests: {completed_requests} / {len(df)}')\n",
    "\n",
    "#             if len(tasks['publish_date']) % MAX_CONCURRENT_REQUESTS == 0:\n",
    "#                 await asyncio.sleep(1)  # Respect the rate limit\n",
    "\n",
    "#         logger.info(\"All requests completed. Gathering results...\")\n",
    "#         results = await asyncio.gather(*[asyncio.gather(*task_list) for task_list in tasks.values()])\n",
    "#         logger.info(\"Results gathered successfully.\")\n",
    "\n",
    "#     logger.info(\"Updating DataFrame with the results...\")\n",
    "#     for idx, field in enumerate(tasks.keys()):\n",
    "#         df[field] = results[idx]\n",
    "#     logger.info(\"DataFrame updated successfully.\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# async def main():\n",
    "#     logger.info(\"Starting to fetch book information...\")\n",
    "#     updated_df = await fetch_book_info_for_dataframe(df1)\n",
    "#     logger.info(\"Finished fetching book information.\")\n",
    "#     print(updated_df.head())\n",
    "\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aiohttp\n",
    "# import asyncio\n",
    "# import pandas as pd\n",
    "# import nest_asyncio\n",
    "# from asyncio import Semaphore\n",
    "# import logging\n",
    "\n",
    "# # Apply the nest_asyncio to allow nested event loops\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Enable logging for debugging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Set the maximum number of concurrent requests and rate limit adjustments\n",
    "# MAX_CONCURRENT_REQUESTS = 1000  # Adjust concurrency based on your needs\n",
    "# semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# # Function to fetch publication year from Open Library\n",
    "# async def fetch_data(session, isbn, field):\n",
    "#     async with semaphore:\n",
    "#         url = f'https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}&format=json&jscmd=data'\n",
    "#         try:\n",
    "#             async with session.get(url) as response:\n",
    "#                 logger.info(f'Fetching ISBN: {isbn} with status: {response.status}')\n",
    "#                 if response.status == 429:  # Rate limit exceeded\n",
    "#                     logger.warning(f'Rate limit hit for ISBN: {isbn}, retrying after 120 seconds')\n",
    "#                     await asyncio.sleep(120)  # Wait before retrying\n",
    "#                     return await fetch_data(session, isbn, field)  # Retry\n",
    "#                 elif response.status == 200:\n",
    "#                     data = await response.json()\n",
    "#                     key = f'ISBN:{isbn}'\n",
    "#                     if key in data:\n",
    "#                         item = data[key]\n",
    "#                         if field == 'publish_date':\n",
    "#                             publish_date = item.get(field, 'No date found')\n",
    "#                             return publish_date.split('-')[0] if publish_date != 'No date found' else 'No date found'\n",
    "#                         else:\n",
    "#                             return item.get(field, 'No data found')\n",
    "#                     else:\n",
    "#                         return 'No data found'\n",
    "#                 else:\n",
    "#                     logger.error(f'Error fetching ISBN: {isbn}, status code: {response.status}')\n",
    "#                     return 'Error fetching data'\n",
    "#         except Exception as e:\n",
    "#             logger.error(f'Exception occurred for ISBN: {isbn}: {e}')\n",
    "#             return 'Error fetching data'\n",
    "\n",
    "# # Function to fetch book information for a DataFrame\n",
    "# async def fetch_book_info_for_dataframe(df):\n",
    "#     logger.debug(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "#     if 'isbn' not in df.columns:\n",
    "#         raise ValueError(\"The DataFrame does not contain an 'isbn' column.\")\n",
    "\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         tasks = {\n",
    "#             'publish_date': [],\n",
    "#             'authors': [],\n",
    "#             'title': []\n",
    "#         }\n",
    "        \n",
    "#         for isbn in df['isbn']:\n",
    "#             try:\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(fetch_data(session, isbn, field))\n",
    "#             except ValueError as e:\n",
    "#                 logger.error(f\"Skipping ISBN: {isbn} due to error: {e}\")\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(asyncio.sleep(0))  # Placeholder for skipped tasks\n",
    "\n",
    "#             completed_requests = sum(len(task_list) for task_list in tasks.values())\n",
    "#             logger.debug(f'Completed requests: {completed_requests} / {len(df)}')\n",
    "\n",
    "#             if len(tasks['publish_date']) % MAX_CONCURRENT_REQUESTS == 0:\n",
    "#                 await asyncio.sleep(1)  # Respect the rate limit\n",
    "\n",
    "#         results = await asyncio.gather(*[asyncio.gather(*task_list) for task_list in tasks.values()])\n",
    "\n",
    "#     for idx, field in enumerate(tasks.keys()):\n",
    "#         df[field] = results[idx]\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# async def main():\n",
    "#     logger.info(\"Starting to fetch book information...\")\n",
    "#     updated_df = await fetch_book_info_for_dataframe(df1)\n",
    "#     logger.info(\"Finished fetching book information.\")\n",
    "#     print(updated_df.head())\n",
    "\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aiohttp\n",
    "# import asyncio\n",
    "# import pandas as pd\n",
    "# import nest_asyncio\n",
    "# from asyncio import Semaphore\n",
    "# import logging\n",
    "\n",
    "# # Apply the nest_asyncio to allow nested event loops\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Enable logging for debugging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Set the maximum number of concurrent requests and rate limit adjustments\n",
    "# MAX_CONCURRENT_REQUESTS = 40  # Adjusted concurrency to avoid hitting rate limits too quickly\n",
    "# semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# # Adjusted delay time for rate limiting\n",
    "# RATE_LIMIT_DELAY = 120  # Delay time between batches of requests\n",
    "\n",
    "# # Your Google API key\n",
    "# API_KEY = \"AIzaSyDcAxovkBpRGJgsR6BGTZCGodOHmoU2oEM\"\n",
    "\n",
    "# # Function to fetch publication year\n",
    "# async def fetch_data(session, isbn, field, retries=5, retry_delay=RATE_LIMIT_DELAY):\n",
    "#     async with semaphore:\n",
    "#         url = f'https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}&key={API_KEY}'\n",
    "#         try:\n",
    "#             async with session.get(url) as response:\n",
    "#                 logger.info(f'Fetching ISBN: {isbn} with status: {response.status}')\n",
    "                \n",
    "#                 if response.status == 429:  # Rate limit exceeded\n",
    "#                     if retries > 0:\n",
    "#                         logger.warning(f'Rate limit hit for ISBN: {isbn}, retrying after {retry_delay} seconds')\n",
    "#                         await asyncio.sleep(retry_delay)  # Wait before retrying\n",
    "#                         return await fetch_data(session, isbn, field, retries-1, retry_delay)  # Retry with decremented retries\n",
    "#                     else:\n",
    "#                         logger.error(f'Exceeded maximum retries for ISBN: {isbn}')\n",
    "#                         return 'Rate limit exceeded'\n",
    "                \n",
    "#                 elif response.status == 200:\n",
    "#                     data = await response.json()\n",
    "#                     if 'items' in data:\n",
    "#                         item = data['items'][0]['volumeInfo']\n",
    "#                         if field == 'publishedDate':\n",
    "#                             published_date = item.get(field, 'No date found')\n",
    "#                             return published_date.split('-')[0] if published_date != 'No date found' else 'No date found'\n",
    "#                         else:\n",
    "#                             return item.get(field, 'No data found')\n",
    "#                     else:\n",
    "#                         return 'No data found'\n",
    "#                 else:\n",
    "#                     logger.error(f'Error fetching ISBN: {isbn}, status code: {response.status}')\n",
    "#                     return 'Error fetching data'\n",
    "                    \n",
    "#         except Exception as e:\n",
    "#             logger.error(f'Exception occurred for ISBN: {isbn}: {e}')\n",
    "#             return 'Error fetching data'\n",
    "\n",
    "# # Function to fetch book information for a DataFrame\n",
    "# async def fetch_book_info_for_dataframe(df):\n",
    "#     # Print DataFrame columns for debugging\n",
    "#     logger.debug(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "#     if 'isbn' not in df.columns:\n",
    "#         raise ValueError(\"The DataFrame does not contain an 'isbn' column.\")\n",
    "\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         tasks = {\n",
    "#             'publishing_year': [],\n",
    "#             'annotation': [],\n",
    "#             'genre': []\n",
    "#         }\n",
    "        \n",
    "#         # Create tasks for each ISBN\n",
    "#         for isbn in df['isbn']:\n",
    "#             try:\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(fetch_data(session, isbn, field))\n",
    "#             except ValueError as e:\n",
    "#                 logger.error(f\"Skipping ISBN: {isbn} due to error: {e}\")\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(asyncio.sleep(0))  # Placeholder for skipped tasks\n",
    "\n",
    "#             # Log progress\n",
    "#             completed_requests = sum(len(task_list) for task_list in tasks.values())\n",
    "#             logger.debug(f'Completed requests: {completed_requests} / {len(df)}')\n",
    "\n",
    "#             if len(tasks['publishing_year']) % MAX_CONCURRENT_REQUESTS == 0:\n",
    "#                 await asyncio.sleep(1)  # Respect the rate limit\n",
    "\n",
    "#         # Gather results\n",
    "#         results = await asyncio.gather(*[asyncio.gather(*task_list) for task_list in tasks.values()])\n",
    "\n",
    "#     # Add the results to the DataFrame\n",
    "#     for idx, field in enumerate(tasks.keys()):\n",
    "#         df[field] = results[idx]\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# async def main():\n",
    "#     logger.info(\"Starting to fetch book information...\")\n",
    "#     updated_df = await fetch_book_info_for_dataframe(df1)\n",
    "#     logger.info(\"Finished fetching book information.\")\n",
    "#     print(updated_df.head())  # Display the updated DataFrame\n",
    "\n",
    "# # Run the asyncio event loop\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv('df1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aiohttp\n",
    "# import asyncio\n",
    "# import pandas as pd\n",
    "# import nest_asyncio\n",
    "# from asyncio import Semaphore\n",
    "# import logging\n",
    "\n",
    "# # Apply the nest_asyncio to allow nested event loops\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Enable logging for debugging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Set the maximum number of concurrent requests and rate limit adjustments\n",
    "# MAX_CONCURRENT_REQUESTS = 40  # Adjusted concurrency to avoid hitting rate limits too quickly\n",
    "# semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# # Adjusted delay time for rate limiting\n",
    "# RATE_LIMIT_DELAY = 120  # Delay time between batches of requests\n",
    "\n",
    "# # Your Google API key\n",
    "# # API_KEY = \"AIzaSyDcAxovkBpRGJgsR6BGTZCGodOHmoU2oEM\"\n",
    "\n",
    "# # Function to fetch publication year\n",
    "# async def fetch_data(session, isbn, field):\n",
    "#     async with semaphore:\n",
    "#         url = f'https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}'\n",
    "#         try:\n",
    "#             async with session.get(url) as response:\n",
    "#                 logger.info(f'Fetching ISBN: {isbn} with status: {response.status}')\n",
    "#                 if response.status == 429:  # Rate limit exceeded\n",
    "#                     logger.warning(f'Rate limit hit for ISBN: {isbn}, retrying after {RATE_LIMIT_DELAY} seconds')\n",
    "#                     await asyncio.sleep(RATE_LIMIT_DELAY)  # Wait before retrying\n",
    "#                     return await fetch_data(session, isbn, field)  # Retry\n",
    "#                 elif response.status == 200:\n",
    "#                     data = await response.json()\n",
    "#                     if 'items' in data:\n",
    "#                         item = data['items'][0]['volumeInfo']\n",
    "#                         if field == 'publishedDate':\n",
    "#                             published_date = item.get(field, 'No date found')\n",
    "#                             return published_date.split('-')[0] if published_date != 'No date found' else 'No date found'\n",
    "#                         else:\n",
    "#                             return item.get(field, 'No data found')\n",
    "#                     else:\n",
    "#                         return 'No data found'\n",
    "#                 else:\n",
    "#                     logger.error(f'Error fetching ISBN: {isbn}, status code: {response.status}')\n",
    "#                     return 'Error fetching data'\n",
    "#         except Exception as e:\n",
    "#             logger.error(f'Exception occurred for ISBN: {isbn}: {e}')\n",
    "#             return 'Error fetching data'\n",
    "\n",
    "# # Function to fetch book information for a DataFrame\n",
    "# async def fetch_book_info_for_dataframe(df):\n",
    "#     # Print DataFrame columns for debugging\n",
    "#     logger.debug(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "#     if 'isbn' not in df.columns:\n",
    "#         raise ValueError(\"The DataFrame does not contain an 'isbn' column.\")\n",
    "\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         tasks = {\n",
    "#             'publishing_year': [],\n",
    "#             'annotation': [],\n",
    "#             'genre': []\n",
    "#         }\n",
    "        \n",
    "#         # Create tasks for each ISBN\n",
    "#         for isbn in df['isbn']:\n",
    "#             try:\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(fetch_data(session, isbn, field))\n",
    "#             except ValueError as e:\n",
    "#                 logger.error(f\"Skipping ISBN: {isbn} due to error: {e}\")\n",
    "#                 for field in tasks.keys():\n",
    "#                     tasks[field].append(asyncio.sleep(0))  # Placeholder for skipped tasks\n",
    "\n",
    "#             # Log progress\n",
    "#             completed_requests = sum(len(task_list) for task_list in tasks.values())\n",
    "#             logger.debug(f'Completed requests: {completed_requests} / {len(df)}')\n",
    "\n",
    "#             if len(tasks['publishing_year']) % MAX_CONCURRENT_REQUESTS == 0:\n",
    "#                 await asyncio.sleep(1)  # Respect the rate limit\n",
    "\n",
    "#         # Gather results\n",
    "#         results = await asyncio.gather(*[asyncio.gather(*task_list) for task_list in tasks.values()])\n",
    "\n",
    "#     # Add the results to the DataFrame\n",
    "#     for idx, field in enumerate(tasks.keys()):\n",
    "#         df[field] = results[idx]\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# async def main():\n",
    "#     logger.info(\"Starting to fetch book information...\")\n",
    "#     updated_df = await fetch_book_info_for_dataframe(df1)\n",
    "#     logger.info(\"Finished fetching book information.\")\n",
    "#     print(updated_df.head())  # Display the updated DataFrame\n",
    "\n",
    "# # Run the asyncio event loop\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aiohttp\n",
    "# import asyncio\n",
    "# import pandas as pd\n",
    "# import nest_asyncio\n",
    "# from asyncio import Semaphore\n",
    "# import logging\n",
    "\n",
    "# # Apply the nest_asyncio to allow nested event loops\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Enable logging for debugging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# # Set the maximum number of concurrent requests (adjust based on the API rate limit)\n",
    "# MAX_CONCURRENT_REQUESTS = 5\n",
    "# semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "# # Function to convert ISBN-10 to ISBN-13\n",
    "# def isbn10_to_isbn13(isbn10):\n",
    "#     isbn10 = isbn10.replace('-', '')\n",
    "    \n",
    "#     if len(isbn10) != 10 or not (isbn10[:-1].isdigit() and (isbn10[-1].isdigit() or isbn10[-1].upper() == 'X')):\n",
    "#         raise ValueError(\"Invalid ISBN-10 format: ISBN-10 must end with a digit or 'X'\")\n",
    "    \n",
    "#     isbn13_body = '978' + isbn10[:-1]\n",
    "    \n",
    "#     checksum = 0\n",
    "#     for i, digit in enumerate(isbn13_body):\n",
    "#         if i % 2 == 0:\n",
    "#             checksum += int(digit)\n",
    "#         else:\n",
    "#             checksum += 3 * int(digit)\n",
    "    \n",
    "#     checksum = (10 - (checksum % 10)) % 10\n",
    "    \n",
    "#     isbn13 = isbn13_body + str(checksum)\n",
    "    \n",
    "#     return isbn13\n",
    "\n",
    "# # Function to fetch the publication year\n",
    "# async def fetch_publishing_year(session, isbn13):\n",
    "#     async with semaphore:\n",
    "#         url = f'https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn13}'\n",
    "#         try:\n",
    "#             async with session.get(url) as response:\n",
    "#                 logger.info(f'Fetching ISBN: {isbn13} with status: {response.status}')\n",
    "#                 if response.status == 429:  # Too many requests\n",
    "#                     logger.warning(f'Rate limit hit for ISBN: {isbn13}, retrying after 60 seconds')\n",
    "#                     await asyncio.sleep(60)  # Wait 60 seconds and try again\n",
    "#                     return await fetch_publishing_year(session, isbn13)  # Retry\n",
    "#                 elif response.status == 200:\n",
    "#                     data = await response.json()\n",
    "#                     logger.debug(f'Received data for ISBN: {isbn13}: {data}')\n",
    "#                     if 'items' in data:\n",
    "#                         published_date = data['items'][0]['volumeInfo'].get('publishedDate', 'No date found')\n",
    "#                         if published_date != 'No date found':\n",
    "#                             publishing_year = published_date.split('-')[0]\n",
    "#                         else:\n",
    "#                             publishing_year = 'No date found'\n",
    "#                         return publishing_year\n",
    "#                     else:\n",
    "#                         return 'No date found'\n",
    "#                 else:\n",
    "#                     logger.error(f'Error fetching ISBN: {isbn13}, status code: {response.status}')\n",
    "#                     return 'Error fetching date'\n",
    "#         except Exception as e:\n",
    "#             logger.error(f'Exception occurred for ISBN: {isbn13}: {e}')\n",
    "#             return 'Error fetching date'\n",
    "\n",
    "# # Function to fetch the annotation\n",
    "# async def fetch_annotation(session, isbn13):\n",
    "#     async with semaphore:\n",
    "#         url = f'https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn13}'\n",
    "#         try:\n",
    "#             async with session.get(url) as response:\n",
    "#                 logger.info(f'Fetching ISBN: {isbn13} with status: {response.status}')\n",
    "#                 if response.status == 429:  # Too many requests\n",
    "#                     logger.warning(f'Rate limit hit for ISBN: {isbn13}, retrying after 60 seconds')\n",
    "#                     await asyncio.sleep(60)  # Wait 60 seconds and try again\n",
    "#                     return await fetch_annotation(session, isbn13)  # Retry\n",
    "#                 elif response.status == 200:\n",
    "#                     data = await response.json()\n",
    "#                     logger.debug(f'Received data for ISBN: {isbn13}: {data}')\n",
    "#                     if 'items' in data:\n",
    "#                         annotation = data['items'][0]['volumeInfo'].get('description', 'No annotation found')\n",
    "#                         return annotation if annotation else 'No annotation found'\n",
    "#                     else:\n",
    "#                         return 'No annotation found'\n",
    "#                 else:\n",
    "#                     logger.error(f'Error fetching ISBN: {isbn13}, status code: {response.status}')\n",
    "#                     return 'Error fetching annotation'\n",
    "#         except Exception as e:\n",
    "#             logger.error(f'Exception occurred for ISBN: {isbn13}: {e}')\n",
    "#             return 'Error fetching annotation'\n",
    "\n",
    "# # Function to fetch the genre\n",
    "# async def fetch_genre(session, isbn13):\n",
    "#     async with semaphore:\n",
    "#         url = f'https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn13}'\n",
    "#         try:\n",
    "#             async with session.get(url) as response:\n",
    "#                 logger.info(f'Fetching ISBN: {isbn13} with status: {response.status}')\n",
    "#                 if response.status == 429:  # Too many requests\n",
    "#                     logger.warning(f'Rate limit hit for ISBN: {isbn13}, retrying after 60 seconds')\n",
    "#                     await asyncio.sleep(60)  # Wait 60 seconds and try again\n",
    "#                     return await fetch_genre(session, isbn13)  # Retry\n",
    "#                 elif response.status == 200:\n",
    "#                     data = await response.json()\n",
    "#                     logger.debug(f'Received data for ISBN: {isbn13}: {data}')\n",
    "#                     if 'items' in data:\n",
    "#                         genres = data['items'][0]['volumeInfo'].get('categories', ['No genre found'])\n",
    "#                         return ', '.join(genres) if genres else 'No genre found'\n",
    "#                     else:\n",
    "#                         return 'No genre found'\n",
    "#                 else:\n",
    "#                     logger.error(f'Error fetching ISBN: {isbn13}, status code: {response.status}')\n",
    "#                     return 'Error fetching genre'\n",
    "#         except Exception as e:\n",
    "#             logger.error(f'Exception occurred for ISBN: {isbn13}: {e}')\n",
    "#             return 'Error fetching genre'\n",
    "\n",
    "# # Function to fetch publication year, annotation, and genre for a DataFrame\n",
    "# async def fetch_book_info_for_dataframe(books_big, rate_limit=5):\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         tasks_year = []\n",
    "#         tasks_annotation = []\n",
    "#         tasks_genre = []\n",
    "        \n",
    "#         for isbn in books_big['isbn']:\n",
    "#             try:\n",
    "#                 # Convert to ISBN-13 if it's ISBN-10\n",
    "#                 if len(isbn.replace('-', '')) == 10:\n",
    "#                     isbn13 = isbn10_to_isbn13(isbn)\n",
    "#                 else:\n",
    "#                     isbn13 = isbn\n",
    "                \n",
    "#                 # Create tasks for each type of data to fetch\n",
    "#                 tasks_year.append(fetch_publishing_year(session, isbn13))\n",
    "#                 tasks_annotation.append(fetch_annotation(session, isbn13))\n",
    "#                 tasks_genre.append(fetch_genre(session, isbn13))\n",
    "                \n",
    "#             except ValueError as e:\n",
    "#                 logger.error(f\"Skipping ISBN: {isbn} due to error: {e}\")\n",
    "#                 # Add placeholders for skipped tasks\n",
    "#                 tasks_year.append(asyncio.sleep(0))\n",
    "#                 tasks_annotation.append(asyncio.sleep(0))\n",
    "#                 tasks_genre.append(asyncio.sleep(0))\n",
    "            \n",
    "#             if len(tasks_year) % rate_limit == 0:  # After every 'rate_limit' requests\n",
    "#                 await asyncio.sleep(1)  # Sleep to respect the rate limit\n",
    "\n",
    "#         # Gather results for each type of data\n",
    "#         publishing_years = await asyncio.gather(*tasks_year)\n",
    "#         annotations = await asyncio.gather(*tasks_annotation)\n",
    "#         genres = await asyncio.gather(*tasks_genre)\n",
    "\n",
    "#     # Add the results to the DataFrame\n",
    "#     books_big['publishing_year'] = publishing_years\n",
    "#     books_big['annotation'] = annotations\n",
    "#     books_big['genre'] = genres\n",
    "    \n",
    "#     return books_big\n",
    "\n",
    "# async def main():\n",
    "#     global books_big\n",
    "#     # Assuming books_big is your existing DataFrame with an 'isbn' column (which can be ISBN-10 or ISBN-13)\n",
    "#     books_big = await fetch_book_info_for_dataframe(books_big)\n",
    "#     print(books_big.head())  # Display the updated DataFrame with publication year, annotation, and genre\n",
    "\n",
    "# # Instead of asyncio.run(main()), we use an event loop directly:\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books_big.annotation.nunique())\n",
    "print(books_big.publishing_year.nunique())\n",
    "print(books_big.genre.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books_big.to_csv('books_with_annotations.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
