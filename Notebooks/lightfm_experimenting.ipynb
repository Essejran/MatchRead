{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightfm\n",
      "  Using cached lightfm-1.17-cp311-cp311-macosx_14_0_arm64.whl\n",
      "Collecting numpy (from lightfm)\n",
      "  Using cached numpy-2.1.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting scipy>=0.17.0 (from lightfm)\n",
      "  Using cached scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting requests (from lightfm)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scikit-learn (from lightfm)\n",
      "  Using cached scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->lightfm)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->lightfm)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->lightfm)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->lightfm)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->lightfm)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->lightfm)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Using cached numpy-2.1.1-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, threadpoolctl, numpy, joblib, idna, charset-normalizer, certifi, scipy, requests, scikit-learn, lightfm\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.5.0\n",
      "    Uninstalling threadpoolctl-3.5.0:\n",
      "      Successfully uninstalled threadpoolctl-3.5.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.1\n",
      "    Uninstalling numpy-2.1.1:\n",
      "      Successfully uninstalled numpy-2.1.1\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "  Attempting uninstall: lightfm\n",
      "    Found existing installation: lightfm 1.17\n",
      "    Uninstalling lightfm-1.17:\n",
      "      Successfully uninstalled lightfm-1.17\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.1 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.1 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
      "tensorflow 2.16.2 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2024.8.30 charset-normalizer-3.3.2 idna-3.10 joblib-1.4.2 lightfm-1.17 numpy-2.1.1 requests-2.32.3 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "# Reinstall lightfm with gcc-14 compiled to use all threads\n",
    "!CC=gcc-14 pip install --no-binary lightfm lightfm --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from lightfm import LightFM\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n",
    "from lightfm.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodreads dataset loaded successfully as books_goodreads\n",
      "Pandas dataframes (books_goodreads, books_big, book, users, ratings) loaded successfully\n",
      "Columns in DataFrames 'users' and 'ratings' renamed\n",
      "You can use the DataFrames 'books' or 'books_big' - they are exactly the same (big) dataset\n",
      "loading books_ratings and books_users_ratings\n",
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# all_cleaned = pd.read_csv('../data/all_cleaned.csv', usecols=['user_id', 'isbn', 'book_rating'])\n",
    "%run import_data_.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'user_id' and 'isbn' to strings to ensure compatibility with LightFM\n",
    "books_users_ratings['user_id'] = books_users_ratings['user_id'].astype(int)\n",
    "books_users_ratings['user_id'] = books_users_ratings['user_id'].astype(str)\n",
    "books_users_ratings['isbn'] = books_users_ratings['isbn'].astype(str)\n",
    "books_users_ratings['year_of_publication'] = books_users_ratings['year_of_publication'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LightFM dataset object\n",
    "dataset = Dataset()\n",
    "\n",
    "# Fit the dataset to include all unique users and items\n",
    "dataset.fit(\n",
    "    users = books_users_ratings['user_id'].unique(),\n",
    "    items = books_users_ratings['isbn'].unique()\n",
    ")\n",
    "\n",
    "# Build the user-item interaction matrix based on explicit feedback (book_rating)\n",
    "(interactions, weights) = dataset.build_interactions(\n",
    "    [(x[0], x[1], x[2]) for x in books_users_ratings[['user_id', 'isbn', 'individual_rating']].values]\n",
    ")\n",
    "\n",
    "# Get the user and item mappings\n",
    "user_mapping, _, item_mapping, _ = dataset.mapping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x321324b90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the LightFM model using the logistic loss function for explicit feedback\n",
    "model = LightFM(loss='warp')\n",
    "\n",
    "# Train the model on the interactions matrix\n",
    "model.fit(interactions, epochs=30, num_threads=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../streamlit_files/lightfm_model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained LightFM model\n",
    "joblib.dump(model, '../streamlit_files/lightfm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(model, interactions, user_id, user_mapping, item_mapping, num_recommendations=10):\n",
    "    # Ensure the user_id is a string\n",
    "    user_id = str(user_id)\n",
    "\n",
    "    # Check if the user_id exists in the user mapping\n",
    "    if user_id not in user_mapping:\n",
    "        raise ValueError(f\"User ID {user_id} is not found in the dataset.\")\n",
    "\n",
    "    # Get the internal index for the user_id\n",
    "    user_idx = user_mapping[user_id]\n",
    "\n",
    "    # Predict scores for all items for the given user\n",
    "    scores = model.predict(user_idx, np.arange(interactions.shape[1]))\n",
    "\n",
    "    # Get the indices of the top scores\n",
    "    top_items = np.argsort(-scores)[:num_recommendations]\n",
    "\n",
    "    # Map the indices back to ISBNs\n",
    "    recommended_isbns = [list(item_mapping.keys())[list(item_mapping.values()).index(item)] for item in top_items]\n",
    "\n",
    "    return recommended_isbns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_recommend = '16634'  # Replace with an actual user_id from your dataset\n",
    "\n",
    "# Ensure the user ID is a string\n",
    "user_id_to_recommend = str(user_id_to_recommend)\n",
    "\n",
    "recommended_books = recommend_books(model, interactions, user_id_to_recommend, user_mapping, item_mapping)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
